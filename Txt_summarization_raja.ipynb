{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rei/.local/lib/python3.11/site-packages (from datasets) (1.26.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/rei/.local/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from huggingface-hub>=0.18.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from huggingface-hub>=0.18.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rei/.local/lib/python3.11/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rei/.local/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rei/.local/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install pytorch::pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print\n",
    "from pprint import pprint\n",
    "# Datasets load_dataset function\n",
    "from datasets import load_dataset\n",
    "# Transformers Autokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Standard PyTorch DataLoader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rei/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/rei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/rei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/rei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading is done!\n"
     ]
    }
   ],
   "source": [
    "hupd_dict = load_dataset('HUPD/hupd',\n",
    "    name='sample',\n",
    "    data_files=\"https://huggingface.co/datasets/HUPD/hupd/blob/main/hupd_metadata_2022-02-22.feather\",\n",
    "    icpr_label=None,\n",
    "    train_filing_start_date='2016-01-01',\n",
    "    train_filing_end_date='2016-01-21',\n",
    "    val_filing_start_date='2016-01-22',\n",
    "    val_filing_end_date='2016-01-31',\n",
    ")\n",
    "\n",
    "print('Loading is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_claims = []\n",
    "for i in range(hupd_dict['train'].shape[0]):\n",
    "    claims_text = hupd_dict['train'][i]['claims']\n",
    "    all_claims.append(claims_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopwords = set(stopwords.words('english'))\n",
    "wordlemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "       lemmatized_words.append(wordlemmatizer.lemmatize(word))\n",
    "    return lemmatized_words\n",
    "def stem_words(words):\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "       stemmed_words.append(stemmer.stem(word))\n",
    "    return stemmed_words\n",
    "def remove_special_characters(text):\n",
    "    regex = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(regex,'',text)\n",
    "    return text\n",
    "def freq(words):\n",
    "    words = [word.lower() for word in words]\n",
    "    dict_freq = {}\n",
    "    words_unique = []\n",
    "    for word in words:\n",
    "       if word not in words_unique:\n",
    "           words_unique.append(word)\n",
    "    for word in words_unique:\n",
    "       dict_freq[word] = words.count(word)\n",
    "    return dict_freq\n",
    "def pos_tagging(text):\n",
    "    pos_tag = nltk.pos_tag(text.split())\n",
    "    pos_tagged_noun_verb = []\n",
    "    for word,tag in pos_tag:\n",
    "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
    "             pos_tagged_noun_verb.append(word)\n",
    "    return pos_tagged_noun_verb\n",
    "def tf_score(word,sentence):\n",
    "    freq_sum = 0\n",
    "    word_frequency_in_sentence = 0\n",
    "    len_sentence = len(sentence)\n",
    "    for word_in_sentence in sentence.split():\n",
    "        if word == word_in_sentence:\n",
    "            word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
    "    tf =  word_frequency_in_sentence/ len_sentence\n",
    "    return tf\n",
    "def idf_score(no_of_sentences,word,sentences):\n",
    "    no_of_sentence_containing_word = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = remove_special_characters(str(sentence))\n",
    "        sentence = re.sub(r'\\d+', '', sentence)\n",
    "        sentence = sentence.split()\n",
    "        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n",
    "        if word in sentence:\n",
    "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
    "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
    "    return idf\n",
    "def tf_idf_score(tf,idf):\n",
    "    return tf*idf\n",
    "def word_tfidf(dict_freq,word,sentences,sentence):\n",
    "    word_tfidf = []\n",
    "    tf = tf_score(word,sentence)\n",
    "    idf = idf_score(len(sentences),word,sentences)\n",
    "    tf_idf = tf_idf_score(tf,idf)\n",
    "    return tf_idf\n",
    "def sentence_importance(sentence,dict_freq,sentences):\n",
    "     sentence_score = 0\n",
    "     sentence = remove_special_characters(str(sentence)) \n",
    "     sentence = re.sub(r'\\d+', '', sentence)\n",
    "     pos_tagged_sentence = [] \n",
    "     no_of_sentences = len(sentences)\n",
    "     pos_tagged_sentence = pos_tagging(sentence)\n",
    "     for word in pos_tagged_sentence:\n",
    "          if word.lower() not in Stopwords and word not in Stopwords and len(word)>1: \n",
    "                word = word.lower()\n",
    "                word = wordlemmatizer.lemmatize(word)\n",
    "                sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
    "     return sentence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original claim:\n",
      " 1. A compact optical network terminal, comprising: a first interface coupled to a communications network; a second interface coupled to a network client, wherein the second interface is a network connectivity dongle with an optical transceiver at one end; and a processor including a circuitry and a memory coupled to the first interface and to the second interface, wherein the processor is capable of converting optical signals to electric signals, such that the network client can access the communications network thereby reducing the unnecessary splitting of equal upstream wavelengths to all the network clients in the network. 2. The optical network terminal of claim 1, wherein the first interface includes an optical module that receives optical signals via the optical fiber link and converts the optical signals to electrical signals. 3. The optical network terminal of claim 2, wherein the optical module is selectively configurable to support two or more of a broadband passive optical network (BPON), a gigabit-capable passive optical network (GPON), an Ethernet passive optical network (EPON), a gigabit-capable Ethernet passive optical network (GEPON) and an active Ethernet optical network. 4. The optical network terminal of claim 1, wherein the network client includes a converter unit for converting at least some of the electrical signals to data units which is selectively configurable to support a plurality of optical network protocols. 5. The optical, network terminal of claim 1, wherein the second interface includes a control circuitry which is capable of receiving the power from the network client for processing the received electrical signals thereby reducing the required protocol processing time from the network client. 6. The optical network terminal of claim 1, wherein the network client is capable of processing the xPON protocols to communicate with one or more Optical Line Terminal (OLT). 7. (canceled) 8. The optical network terminal of claim 1, wherein the second interface includes a Universal Serial Bus (USB) jack which is couplable to Universal Serial Bus (USB) plug or connector of the network client. 9. The optical network terminal of claim 1, wherein the at least one communication service with the xPON network comprises a first communication service and a second communication service, and wherein the communication service includes at least one voice communication service, a data communication service, and a video service. 10. (canceled)\n"
     ]
    }
   ],
   "source": [
    "# Example of generating summaries\n",
    "example_index = 0  # Change this to any index you want to summarize\n",
    "example_text = all_claims[example_index]\n",
    "org_claim = hupd_dict['train'][0]['claims']\n",
    "print(\"Original claim:\\n\", org_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "\n",
      "\n",
      "Summary:\n",
      "A compact optical network terminal, comprising: a first interface coupled to a communications network; a second interface coupled to a network client, wherein the second interface is a network connectivity dongle with an optical transceiver at one end; and a processor including a circuitry and a memory coupled to the first interface and to the second interface, wherein the processor is capable of converting optical signals to electric signals, such that the network client can access the communications network thereby reducing the unnecessary splitting of equal upstream wavelengths to all the network clients in the network. The optical network terminal of claim 2, wherein the optical module is selectively configurable to support two or more of a broadband passive optical network (BPON), a gigabit-capable passive optical network (GPON), an Ethernet passive optical network (EPON), a gigabit-capable Ethernet passive optical network (GEPON) and an active Ethernet optical network. The optical network terminal of claim 1, wherein the network client includes a converter unit for converting at least some of the electrical signals to data units which is selectively configurable to support a plurality of optical network protocols. The optical, network terminal of claim 1, wherein the second interface includes a control circuitry which is capable of receiving the power from the network client for processing the received electrical signals thereby reducing the required protocol processing time from the network client. (canceled) 8. The optical network terminal of claim 1, wherein the at least one communication service with the xPON network comprises a first communication service and a second communication service, and wherein the communication service includes at least one voice communication service, a data communication service, and a video service. (canceled)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1881"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = sent_tokenize(example_text)\n",
    "text = remove_special_characters(str(example_text))\n",
    "text = re.sub(r'\\d+', '', example_text)\n",
    "tokenized_words_with_stopwords = word_tokenize(example_text)\n",
    "tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "tokenized_words = [word.lower() for word in tokenized_words]\n",
    "tokenized_words = lemmatize_words(tokenized_words)\n",
    "word_freq = freq(tokenized_words)\n",
    "input_user = int(input('Percentage of information to retain(in percent):'))\n",
    "no_of_sentences = int((input_user * len(tokenized_sentence))/100)\n",
    "print(no_of_sentences)\n",
    "c = 1\n",
    "sentence_with_importance = {}\n",
    "for sent in tokenized_sentence:\n",
    "    sentenceimp = sentence_importance(sent,word_freq,tokenized_sentence)\n",
    "    sentence_with_importance[c] = sentenceimp\n",
    "    c = c+1\n",
    "sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\n",
    "cnt = 0\n",
    "summary = []\n",
    "sentence_no = []\n",
    "for word_prob in sentence_with_importance:\n",
    "    if cnt < no_of_sentences:\n",
    "        sentence_no.append(word_prob[0])\n",
    "        cnt = cnt+1\n",
    "    else:\n",
    "      break\n",
    "sentence_no.sort()\n",
    "cnt = 1\n",
    "for sentence in tokenized_sentence:\n",
    "    if cnt in sentence_no:\n",
    "       summary.append(sentence)\n",
    "    cnt = cnt+1\n",
    "summary = \" \".join(summary)\n",
    "print(\"\\n\")\n",
    "print(\"Summary:\")\n",
    "print(summary)\n",
    "outF = open('summary.txt',\"w\")\n",
    "outF.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
